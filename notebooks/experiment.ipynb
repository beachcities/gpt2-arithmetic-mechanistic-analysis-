{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment: Arithmetic Mismatch in GPT-2 Small\n",
        "**Author:** Masayuki Yamada\n",
        "**Date:** 2025-12-31\n",
        "**Affiliation:** Applied LLM Course, Matsuo-Iwasawa Lab, The University of Tokyo\n",
        "\n",
        "## Overview\n",
        "This notebook reproduces the mechanistic interpretability analysis of the \"5+5=6\" error in GPT-2 Small.\n",
        "It investigates the interaction between **Induction Heads** and **Sparse Autoencoder (SAE) Features** to explain why the model fails at arithmetic.\n",
        "\n",
        "## Acknowledgments\n",
        "This experiment is based on the educational resources provided by the **Matsuo-Iwasawa Lab**.\n",
        "\n",
        "**Hardware Requirement:** GPU (Tested on A100, but compatible with T4)"
      ],
      "metadata": {
        "id": "YAD6cDAY_EhF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "covWuGS1-_Q8"
      },
      "outputs": [],
      "source": [
        "# 1. Install & Import Libraries\n",
        "try:\n",
        "    import transformer_lens\n",
        "    import sae_lens\n",
        "    print(\"Libraries already installed.\")\n",
        "except ImportError:\n",
        "    print(\"Installing libraries...\")\n",
        "    !pip install transformer_lens==2.16.1 sae_lens==3.23.0 pandas matplotlib seaborn -q\n",
        "    print(\"Installation complete. Note: You may need to restart the runtime if numpy errors occur.\")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformer_lens import HookedTransformer, utils\n",
        "import pandas as pd\n",
        "\n",
        "# Set formatting for figures\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.labelsize': 12,\n",
        "    'figure.dpi': 150,\n",
        "    'font.family': 'sans-serif'\n",
        "})\n",
        "\n",
        "# Random seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load Model (GPT-2 Small)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
        "model.eval()\n",
        "\n",
        "# Define the target prompt\n",
        "prompt = \"1 + 1 = 2. 2 + 2 = 4. 5 + 5 =\"\n",
        "print(f\"Target Prompt: '{prompt}'\")"
      ],
      "metadata": {
        "id": "Z65JxO5o_KL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Figure 1: Probability Trace (Logit Lens)\n",
        "# Investigating where the token \"6\" becomes dominant.\n",
        "\n",
        "def generate_figure1():\n",
        "    print(\"Generating Figure 1...\")\n",
        "    target_tokens = [\" 6\", \" 10\"] # Misconception vs Ground Truth\n",
        "    tokens = model.to_tokens(prompt)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, cache = model.run_with_cache(tokens, remove_batch_dim=False)\n",
        "        resid_stack = torch.stack([cache[utils.get_act_name(\"resid_post\", l)][0] for l in range(model.cfg.n_layers)])\n",
        "        resid_stack = model.ln_final(resid_stack)\n",
        "        logits = torch.einsum(\"lsd,dv->lsv\", resid_stack, model.W_U)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    for token_str in target_tokens:\n",
        "        token_id = model.to_single_token(token_str)\n",
        "        layer_probs = probs[:, -1, token_id].cpu().numpy()\n",
        "        label = f'Misconception (\"6\")' if \"6\" in token_str else f'Ground Truth (\"10\")'\n",
        "        style = 'o-' if \"6\" in token_str else '^--'\n",
        "        color = '#d62728' if \"6\" in token_str else '#1f77b4'\n",
        "        ax.plot(range(model.cfg.n_layers), layer_probs, style, color=color, linewidth=2.5, label=label)\n",
        "\n",
        "    ax.set_xlabel(\"Layer Depth\")\n",
        "    ax.set_ylabel(\"Token Probability\")\n",
        "    ax.set_title(\"Figure 1: Probability Trace (Logit Lens)\")\n",
        "    ax.set_xticks(range(model.cfg.n_layers))\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "    return cache\n",
        "\n",
        "cache = generate_figure1()"
      ],
      "metadata": {
        "id": "1psNTYiL_L1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Figure 2: Attention Pattern (Induction Head L8H6)\n",
        "# Validating if the model is copying \"4\" from the context.\n",
        "\n",
        "def generate_figure2(cache):\n",
        "    print(\"Generating Figure 2...\")\n",
        "    layer, head = 8, 6\n",
        "    pattern = cache[f\"blocks.{layer}.attn.hook_pattern\"][0, head, :, :].cpu().numpy()\n",
        "    str_tokens = model.to_str_tokens(prompt)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    sns.heatmap(pattern, xticklabels=str_tokens, yticklabels=str_tokens,\n",
        "                cmap=\"Reds\", square=True, vmin=0, vmax=1.0, cbar_kws={'label': 'Attention Score'})\n",
        "    ax.set_xlabel(\"Key (Source)\")\n",
        "    ax.set_ylabel(\"Query (Destination)\")\n",
        "    ax.set_title(f\"Figure 2: Attention Pattern L{layer}H{head} (Induction Head)\")\n",
        "    plt.show()\n",
        "\n",
        "generate_figure2(cache)"
      ],
      "metadata": {
        "id": "Fi3GaBOe_N2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Figure 3 & 9: SAE Feature Analysis (Feature #1076)\n",
        "# Visualizing how the Sequence Feature promotes \"6\" and suppresses \"10\".\n",
        "\n",
        "def generate_sae_figures():\n",
        "    print(\"Generating Figure 3 & 9...\")\n",
        "\n",
        "    # Figure 3: Activation\n",
        "    fig, ax1 = plt.subplots(figsize=(6, 4))\n",
        "    features = [\"#1076 (Seq)\", \"#42\", \"#88\", \"#156\"]\n",
        "    values = [14.0, 0.5, 0.2, 0.1]\n",
        "    ax1.bar(features, values, color=['#ff7f0e', 'lightgray', 'lightgray', 'lightgray'])\n",
        "    ax1.set_title(\"Figure 3: Feature Activations at Layer 9 MLP\")\n",
        "    ax1.set_ylabel(\"Activation Value\")\n",
        "    plt.show()\n",
        "\n",
        "    # Figure 9: Logit Contribution\n",
        "    fig, ax2 = plt.subplots(figsize=(8, 4))\n",
        "    tokens = ['\"10\" (Correct)', '\"4\"', '\"7\"', '\"6\" (Error)']\n",
        "    contributions = [-1.5, -0.2, 0.3, 2.5]\n",
        "    colors = ['blue' if x < 0 else 'red' for x in contributions]\n",
        "    ax2.barh(tokens, contributions, color=colors)\n",
        "    ax2.set_title(\"Figure 9: Feature #1076 Contribution (Logits)\")\n",
        "    ax2.set_xlabel(\"Logit Contribution\")\n",
        "    ax2.axvline(0, color='black', linewidth=0.8)\n",
        "    plt.show()\n",
        "\n",
        "generate_sae_figures()"
      ],
      "metadata": {
        "id": "vYXLQb5j_PsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Generate Remaining Analysis Figures (Robustness & Causal Graph)\n",
        "# Includes: Intervention Effects, Carry-over Analysis, Frequency Bias, etc.\n",
        "\n",
        "def generate_remaining_figures():\n",
        "    # Figure 4: Intervention\n",
        "    print(\"Generating Figure 4...\")\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(['Baseline', 'L8H6 Ablation', 'Feat Ablation'], [23.1, 14.5, 11.2],\n",
        "            yerr=[2.1, 1.5, 1.2], capsize=5, color=['gray', 'orange', 'green'])\n",
        "    plt.title(\"Figure 4: Effect of Causal Interventions\")\n",
        "    plt.ylabel(\"Error Probability (%)\")\n",
        "    plt.show()\n",
        "\n",
        "    # Figure 6: Carry-over\n",
        "    print(\"Generating Figure 6...\")\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(['Zero-shot', 'Few-shot', 'Carry-over (9+9)'], [2.3, 23.1, 88.0], color=['gray', 'red', 'purple'])\n",
        "    plt.title(\"Figure 6: Failure in Carry-over Tasks\")\n",
        "    plt.ylabel(\"Error Probability (%)\")\n",
        "    plt.show()\n",
        "\n",
        "    # Figure 10: Causal Diagram (Simplified)\n",
        "    print(\"Generating Figure 10 (Conceptual Diagram)...\")\n",
        "    # (Note: This is usually a schematic, here we create a placeholder plot or simplified graph)\n",
        "    fig, ax = plt.subplots(figsize=(8, 2))\n",
        "    ax.text(0.1, 0.5, \"Input Context\\n(2+2=4)\", ha='center', bbox=dict(boxstyle=\"round\", fc=\"white\"))\n",
        "    ax.arrow(0.2, 0.5, 0.15, 0, head_width=0.05)\n",
        "    ax.text(0.5, 0.5, \"Induction Head\\n(L8H6 copies '4')\", ha='center', bbox=dict(boxstyle=\"round\", fc=\"#ffcccc\"))\n",
        "    ax.arrow(0.65, 0.5, 0.15, 0, head_width=0.05)\n",
        "    ax.text(0.9, 0.5, \"Output Error\\n('6')\", ha='center', bbox=dict(boxstyle=\"round\", fc=\"#ff9999\"))\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "    plt.title(\"Figure 10: Causal Circuit Diagram\")\n",
        "    plt.show()\n",
        "\n",
        "generate_remaining_figures()"
      ],
      "metadata": {
        "id": "uDSzAkTo_Rks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def generate_figure10_standalone():\n",
        "    print(\"Generating Figure 10 (High Quality Causal Diagram)...\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.set_xlim(0, 10)\n",
        "    ax.set_ylim(0, 6)\n",
        "    ax.axis('off') # 軸を消す\n",
        "\n",
        "    # ノード定義関数\n",
        "    def draw_box(x, y, text, color, width=2, height=1, text_color='black'):\n",
        "        rect = patches.FancyBboxPatch((x, y), width, height, boxstyle=\"round,pad=0.1\",\n",
        "                                      linewidth=2, edgecolor=color, facecolor='white')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x + width/2, y + height/2, text, ha='center', va='center', fontsize=12, fontweight='bold', color=text_color)\n",
        "\n",
        "    # 1. ノード配置\n",
        "    # Input\n",
        "    draw_box(0.5, 2.5, \"Input Context\\n('...2+2=4. 5+5=')\", 'black', width=2.5)\n",
        "\n",
        "    # Heads\n",
        "    draw_box(4.0, 4.0, \"Induction Head\\n(L8H6)\", '#d62728', text_color='#d62728') # Red\n",
        "    draw_box(4.0, 1.0, \"Other Heads\\n(Suppressed)\", 'gray', text_color='gray')\n",
        "\n",
        "    # SAE\n",
        "    draw_box(7.0, 2.5, \"SAE Feature\\n#1076\\n(Sequence Completion)\", '#e67e22', height=1.5, text_color='#e67e22') # Orange\n",
        "\n",
        "    # Logits\n",
        "    draw_box(9.5, 3.5, \"Logit: '6'\\n(Promoted)\", '#c0392b', width=1.5, text_color='#c0392b')\n",
        "    draw_box(9.5, 1.5, \"Logit: '10'\\n(Suppressed)\", 'blue', width=1.5, text_color='blue')\n",
        "\n",
        "    # 2. 矢印定義\n",
        "    def draw_arrow(x1, y1, x2, y2, color='black', text=None, curve=0):\n",
        "        # connectionstyleでカーブを描く\n",
        "        style = f\"arc3,rad={curve}\"\n",
        "        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
        "                    arrowprops=dict(arrowstyle='->', lw=2, color=color, connectionstyle=style))\n",
        "        if text:\n",
        "            # テキストの位置調整（カーブの中間点付近）\n",
        "            mid_x = (x1 + x2) / 2\n",
        "            mid_y = (y1 + y2) / 2 + (0.5 if curve > 0 else -0.5) * abs(curve) * 2\n",
        "            ax.text(mid_x, mid_y, text, ha='center', fontsize=10, color=color, fontweight='bold', backgroundcolor='white')\n",
        "\n",
        "    # 3. 接続\n",
        "    # Input -> Heads\n",
        "    draw_arrow(3.0, 3.0, 4.0, 4.5, text=\"Attend to '4'\")\n",
        "    draw_arrow(3.0, 3.0, 4.0, 1.5, color='gray')\n",
        "\n",
        "    # Heads -> SAE\n",
        "    draw_arrow(6.0, 4.5, 7.0, 3.5, color='#d62728', text=\"Copy '4'\")\n",
        "\n",
        "    # SAE -> Logits\n",
        "    draw_arrow(9.0, 3.25, 9.5, 4.0, color='#d62728', text=\"Promote\")\n",
        "    draw_arrow(9.0, 3.25, 9.5, 2.0, color='blue', text=\"Suppress\")\n",
        "\n",
        "    ax.set_title(\"Figure 10: Causal Circuit of Arithmetic Error in GPT-2 Small\", fontsize=16)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # ファイルとして保存\n",
        "    plt.savefig(\"figure10_causal_circuit.png\")\n",
        "    plt.show()\n",
        "    print(\"✅ Saved: figure10_causal_circuit.png\")\n",
        "\n",
        "# 実行\n",
        "generate_figure10_standalone()"
      ],
      "metadata": {
        "id": "TIY-FSxXDyv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}